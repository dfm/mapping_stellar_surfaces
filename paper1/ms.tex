% !TeX root = ./ms.tex
\documentclass[modern]{aastex62}

% Load the corTeX style definitions
\input{cortex}

% Load custom style
\input{style}

% Bibliography
\bibliographystyle{aasjournal}

\usepackage{etoolbox}
\makeatletter % we need to patch \env@cases that has @ in its name
\patchcmd{\env@cases}{\quad}{\qquad\qquad}{}{}
\makeatother

\defcitealias{PaperII}{Paper II}

\usepackage{enumitem}

\newcommand{\dfm}[1]{{\color{blue}#1}}

% Begin!
\begin{document}

% Title
\title{%
    \textbf{
        Mapping stellar surfaces\\
        I: Degeneracies in the rotational light curve problem
    }
}

% Author list
\author[0000-0002-0296-3826]{Rodrigo Luger}\altaffiliation{Flatiron Fellow}
\email{rluger@flatironinstitute.org}
\affil{Center~for~Computational~Astrophysics,~Flatiron~Institute,~New~York,~NY}
\affil{Virtual~Planetary~Laboratory, University~of~Washington, Seattle, WA}
%
\author[0000-0002-9328-5652]{Daniel Foreman-Mackey}
\affil{Center~for~Computational~Astrophysics,~Flatiron~Institute,~New~York,~NY}
%
\author[0000-0002-3385-8391]{Christina Hedges}
\affil{Bay~Area~Environmental~Research~Institute,~P.O.~Box~25,~Moffett~Field,~CA~94035,~USA}
\affil{NASA~Ames~Research~Center,~Moffett~Field,~CA}
%

\keywords{time series analysis --- light curves --- stellar surfaces --- starspots}

\features{open-source figures \codeicon; equation unit tests: \input{tests/tally}}

\begin{abstract}
    %
    Thanks to missions like \emph{Kepler} and \emph{TESS}, we now
    have access to tens of thousands of high precision, fast
    cadence, and long baseline stellar photometric observations.
    %
    In principle, these light curves encode a vast amount of information about
    stellar variability and, in particular, about the distribution of
    starspots and other features on their surfaces.
    %
    Unfortunately, the problem of inferring stellar surface properties
    from a rotational light curve is famously ill-posed;
    any inference that relies exclusively on the light curve
    of a single star is \emph{necessarily} prior-dominated.
    %
    The goal of this paper is twofold:
    (1) to explore the various degeneracies affecting the stellar
    light curve ``inversion'' problem and their effect on
    what can and cannot be learned from a stellar surface
    given unresolved photometric measurements; and
    (2) to motivate ensemble analyses of the light curves of
    many stars at once as a powerful method of breaking
    these degeneracies.
    %
    \dfm{While the examples in this paper focus on mapping stellar surfaces
        using photometric time series, the conclusions and proposed methods
        apply to more general problems, including applications where the stellar
        surface is a nuisance (exoplanet detection and characterization using
        transit light curves or radial velocities) or to spectral time series
        datasets (transmission spectroscopy and Doppler imaging).}
    %
    This is the first in a series of papers devoted to the development
    of novel algorithms and tools for the analysis of
    stellar light curves and spectral time series, with the
    explicit goal of enabling statistically robust inference
    about their surface properties.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The advent of space-based precision photometry with missions such as
\emph{Kepler} \citep{Borucki2010} and \emph{TESS} \citep{Ricker2015}
has led to a renewed interest in the modeling of stellar light curves,
and, in particular, in understanding what these light curves can tell
us about the surfaces of stars across the HR diagram. One of the dominant
sources of stellar light curve variability is the modulation caused
by starspots rotating in and out of view.
Dark spots arise due to the suppression of convection in regions of
intense magnetic field, resulting in a locally cooler (and hence darker)
photosphere. Bright spots can similarly
arise in the photosphere as faculae or in the chromosphere as plages, and
are also magnetically driven \citep[e.g.,][]{Berdyugina2005}.
%
Constraining the sizes,
contrasts, locations, and number of spots on stars can therefore reveal
information about stellar magnetic activity, stellar interior structure,
and how these quantities vary across spectral type and over time.
A detailed understanding of the stellar surface is also crucial to
mitigating systematics in the radial velocity search for exoplanets
\citep[e.g.,][]{Lanza2011} and in the spectroscopic characterization of their
atmospheres \citep[e.g.,][]{Rackham2018}.

To date, most studies aimed at inferring stellar surface
properties from light curves follow one of two broad approaches. The
first is to model the stellar surface as a collection of one or more
discrete, circular, uniform contrast dark or bright spots on a uniform
intensity photosphere.
The advantage of this approach is that
the light curve can be computed efficiently and in some cases even
analytically \citep[e.g.,][]{Davenport2015,Morris2017,Morris2020b}.
%
The second approach is to discretize the surface at some resolution
and compute the emergent flux as a weighted sum of the visible pixel intensities.
This approach is more flexible, since it is not limited to surfaces
composed of distinct circular spots \citep[e.g.,][]{Harmon2000,Roettenbacher2017}.
%
Both approaches rely on an explicit \emph{forward model}, a prescription for
how to generate data given a set of parameters. In most cases, however, we are interested
in the inverse problem: constraining the parameters given data.
%
Unfortunately, the inverse problem is not only difficult---as it requires
a large number of forward model evaluations to find the parameter values
that are most consistent with the
data---but also formally \emph{ill-posed}.
%
While the mapping from a stellar surface to a light curve (the forward
problem) is unique, the mapping from a light curve to a surface
(the inverse problem) is not:
given any light curve, there exist an infinite number of surfaces that
could have generated it. These degeneracies are illustrated in
Figure~\ref{fig:degeneracies}, where six synthetic stellar surfaces
are shown (rows) at twelve different phases (columns), all at an
inclination $I=60^\circ$. Each surface
consists of a different number of dark spots on a brighter, heterogeneous
background.

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/degeneracies.pdf}
        \oscaption{degeneracies}{%
            The fundamental limitations of the mapping problem. Each row
            corresponds to a stellar surface with a different number of
            dark spots seen at various phases at an inclination $I=60^\circ$;
            all images are shown on the same color scale.
            The bottom panel shows the light curves of each of these stars.
            All six light curves are indistinguishable from each other, even
            at infinite signal to noise. See text for details.
            \label{fig:degeneracies}
        }
    \end{centering}
\end{figure}

While the stellar surfaces are all distinct, containing between one (top)
and six (bottom) large dark spots,
\textbf{their rotational light curves are identical}
(lower panel). This is true even in the absence of measurement
error: the mapping from a stellar surface
to its rotational light curve is so degenerate that there exist an infinite number of solutions
to the inverse problem. This fact has been pointed out recently in different contexts
\citep[e.g.,][]{Cowan2013,Luger2019,Basri2020}, but it dates back at least to
\citet{Russell1906}, who demonstrated it by expanding the surface
intensity of a celestial body in terms of spherical harmonics
(see Figure~\ref{fig:ylms}). \citet{Russell1906} showed
that many of the modes comprising the intensity profile of a spherical
object are in the \emph{null space}, the set of surface features that have identically
zero effect on the light curve. In fact, as we will show in \S\ref{sec:nullspace},
the \emph{vast majority} of the modes are in the null space for rotational
light curves of stars. This is what allows us to construct pathological
scenarios like that shown in Figure~\ref{fig:degeneracies}, where the light curve
could be explained by any number of spots atop a heterogeneous bright background.

Stellar mapping studies tackle these degeneracies in different ways, but
it usually comes down to a choice of prior: when the data is not
sufficiently informative, assumptions\dfm{---either implicit or explicit---}are needed to discriminate
between competing solutions. In discrete spot models like the ones
discussed above, the degeneracy-breaking prior is (typically) the assumption that the
spots must be circular, have uniform contrast, and sit atop an otherwise
uniform photosphere. In gridded stellar surface models, it is common to
assume a regularization prior such as the maximum entropy penalty
\citep[e.g.,][]{Vogt1987}, which typically favors solutions with the fewest
number of dark pixels (usually referred to as the ``simplest'' solution).

While these assumptions may be approximately valid in some cases, it is
important to bear in mind that because of the light curve degeneracies discussed above,
\textbf{most of the information about the stellar surface usually comes from the \dfm{modeling assumptions}},
so it is very important to \dfm{get these assumptions right}. In general, starspots are not circular and do not have uniform
contrast throughout; nor do spots always arrange themselves in the highest
entropy configuration. The amount of bias introduced by these assumptions
will in general vary, but in principle it could be quite significant.

% Add to this photometric noise, the generally unknown stellar inclination,
% the poorly constrained limb darkening parameters, and you get ... a mess.

The goal of this paper is to explore the degeneracies at play in the
stellar surface mapping problem from a theoretical standpoint.
We will focus in particular on two sources of degeneracies: the
null space intrinsic to the mapping from a two-dimensional surface to
a one-dimensional light curve (\S\ref{sec:nullspace})
and the degeneracy due to the unknowability of the true baseline in
single-band photometry (\S\ref{sec:baseline}). Within each section we
will discuss ways to either break these degeneracies or explicitly marginalize
over degenerate quantities.
We will focus in particular on the power of ensemble analyses: the
joint analysis of many light curves of statistically ``similar''
stars. We will show that even though individual light curves are not very
constraining, light curves of many stars
observed at different inclinations can uniquely constrain certain
properties of the surfaces of those stars.
%
This idea was recently explored to some extent
in \citet{Morris2020}, who used ensemble analyses to derive constraints on
spot coverage areas as a function of stellar age. However, one of
the main conclusions of the present paper is that
quantities like \textbf{the
    total spot coverage, the typical spot contrast, and the total number of
    spots are not observables in single-band photometry.} Instead,
any constraints placed on these quantities, even in the context of
ensemble analyses, are \dfm{driven by the choice of prior and other assumptions}. % necessarily coming from the prior.

The present paper is also similar to \citet{Walkowicz2013} and
\citet{Basri2020}, who explored the information content of
stellar light curves from a large set of simulated spotted stellar
surfaces. While our paper is largely complementary to
that work, we instead approach the information content problem from
a purely theoretical---as opposed to empirical---point of view.
%
The present paper is the first in a series dedicated to the development of
techniques to perform robust inference about stellar surfaces from
unresolved photometric and spectroscopic measurements. The results
of this paper serve as the starting point for the development of
an interpretable Gaussian process for the ensemble analysis of
stellar light curves, which is the subject of
\citepalias[\citealt{PaperII}, hereafter][]{PaperII}.

\vspace{1em}

All of the figures in this paper were auto-generated
using the Azure Pipelines continuous integration (CI) service.
Icons next to each of the figures \codeicon \,
link to the exact script used to generate them to ensure the reproducibility
of our results. In this paper we also introduce the concept of equation
``unit tests'': \textsf{pytest}-compatible test scripts associated
with the principal equations that pass (fail) if the equation is correct (wrong),
in which case a clickable \testpassicon \, (\testfailicon) is shown next to the equation
label.
In most cases, the validity of an equation is gauged by comparison to
a numerical solution. Like the figure scripts, the equation unit tests are
run on Azure Pipelines upon every commit of the code.%
\footnote{
    These unit tests are certainly not foolproof: in particular, there is
    no guarantee against a mismatch in the \LaTeX \, version of an equation
    and its \Python implementation (e.g., due to an uncaught typo). However, they
    \emph{do} ensure that the linked \Python implementation is correct to
    within the accuracy of the numerical solution, providing readers with a
    valid implementation of the equation for purposes of reproducibility.
}

\section{The null space}
\label{sec:nullspace}

\dfm{I think it would be useful to have a short summary here.
    Something like: In this section, we define and present some demonstrations of the null space and how it can affect inferences about stellar surfaces.
    We also present some examples of how these issues can be tackled.
    While these specific examples may appear somewhat idealized, at the end of this section, we discuss how these ideas generalize to more realistic scenarios.
    This is also a topic that we discuss in much more detail in \citetalias{PaperII}.}

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/ylms.pdf}
        \oscaption{ylms}{%
            The real spherical harmonics in the polar frame up to
            $l = 5$, where dark colors correspond to negative intensity
            and bright colors to positive intensity.
            Rows correspond to the degree $l$ and columns to
            the order $m$. The set of all spherical harmonics forms a
            complete, orthogonal basis on the sphere.
            \label{fig:ylms}
        }
    \end{centering}
\end{figure}

\subsection{Rank of the flux operator}
%
In general, inferring the properties of a stellar surface from its light curve alone
is not
only difficult, but \emph{formally impossible}. To understand why, consider
an expansion of the stellar surface intensity in the spherical
harmonic basis out to arbitrary order\footnote{\dfm{We note that this expansion is fully general, since spherical harmonics constitute a complete basis on the sphere.}} (see Figure~\ref{fig:ylms}).
Assuming (for the moment) that the
star rotates about an axis that points up along the page, the observed
light curve may be expressed as a weighted sum of the disk-integrated intensity
of each of the spherical harmonics as they rotate about that same axis.
However, not all spherical harmonics will contribute to the full light curve,
as many (in fact, most) of the spherical harmonics are perfectly antisymmetric
about the equator. This is the case for
the $l = 1$, $m = -1$ harmonic, which integrates to zero regardless of
the phase at which it is viewed. The same is true, in fact, for all other harmonics
of order $m = -1$ and (perhaps less obviously) for all harmonics with odd
$l = 3, 5, 7, ...$ Furthermore, there exist many linear combinations of
spherical harmonics that similarly integrate to zero at all rotational
phases. Together, these modes constitute the \emph{null space} of the problem:
the set of modes on the surface that do not contribute to the observed
light curve and therefore cannot be probed from photometry.

For rotational light curves,
the vast majority of the surface modes lie at least partly in the null
space. To show this, we will make use of the fact that we can express
the vector of $K$ observed fluxes $\mathbf{f}$ (i.e., the light curve)
as a linear operation on the vector of $N$
spherical harmonic coefficients $\mathbf{y}$ \citep{Luger2019}:
%
\begin{align}
    \label{eq:fAy}
    \mathbf{f} = \mathbf{A} \, \mathbf{y}
    \quad,
\end{align}
%
where $\mathbf{A}$ is the $(K \times N)$ \emph{design matrix} of the transformation, whose columns
describe how each of the $N$ components in the spherical harmonic basis contribute
to each of the $K$ points in the light curve.%
\footnote{%
    For rotational light curves, the rows of $\mathbf{A}$ are given by the
    quantity $\mathbf{r}^\top \mathbf{A}_1 \mathbf{R}$ in Equation~(18) of
    \citet{Luger2019}, where $\mathbf{r}^\top$ is a vector of disk-integrated
    intensities, $\mathbf{A}_1$ is a change of basis matrix, and $\mathbf{R}$
    is a spherical harmonic rotation matrix that depends on the stellar inclination
    and the rotational phase of the star. Refer to that paper for more details.
}
%
Even though we are explicitly choosing the spherical harmonics as the basis in which
we describe the stellar surface, Equation~(\ref{eq:fAy}) is quite general and applies to
\emph{any} basis that is linearly related to the flux. For instance, $\mathbf{y}$
could instead describe the intensities in each of the $N$ pixels of a gridded stellar
surface, in which case $\mathbf{A}$ would be the matrix of pixel visibilities that describe
how to sum each of the pixels to obtain the observed light curve.

The size of the null space is called the \emph{nullity}, and it is equal to
$N - R$, where $N$ is once again the number of coefficients describing the
stellar surface and $R$ is the \emph{rank} of the flux operator $\mathbf{A}$.
The rank $R$ is the number of linearly independent columns in $\mathbf{A}$, which
can be computed \dfm{numerically using any standard} linear algebra package. It is equal
to the number of independent signals (or components) that can be measured given
an observation of $\mathbf{f}$.

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/rank.pdf}
        \oscaption{rank}{%
            Rank and nullity of the flux operator. The orange curve
            shows the number of spherical harmonic coefficients required
            to fully describe a stellar surface up to a given spherical
            harmonic degree (bottom axis) or, equivalently, up to an
            effective surface resolution (top axis). The blue curve shows
            the rank of the flux operator, corresponding to the maximum
            number of independent signals that can be constrained from a
            light curve. The size of the null space (the nullity) is
            the difference between the two curves.
            \label{fig:rank}
        }
    \end{centering}
\end{figure}

Figure~\ref{fig:rank} shows the rank and nullity of the flux operator
as a function of the resolution of the surface map (quantified as the
spherical harmonic degree $l$ of the expansion). The orange
curve is the number of spherical harmonic coefficients needed to
represent a surface map up to degree $l$, and is equal to $N = (l + 1)^2$.
The blue dots show the rank of the flux operator $\mathbf{A}$ as a function
of $l$, and the blue line is the function $R = 2l + 1$. The nullity is
simply the difference between $N$ and $R$.

The most striking feature in Figure~\ref{fig:rank} is how quickly the two
curves diverge as $l$ increases. What this means for the mapping problem
is that the number of surface modes---the total information needed to
represent a surface map at some resolution---grows much more quickly than the
number of independent signals in the light curve.
%
At all but the lowest resolutions, there are always more features in the
null space than signals one can measure in the light curve, a difference
that grows \emph{quadratically} with $l$.
%
This means that although a light curve can tell us some information about
a stellar surface on very large scales, the amount of information it tells
us quickly decreases for smaller scales and all but vanishes for the smallest
surface features.

It is also worth noting the piecewise nature of the rank as a function of
$l$: increasing the degree of the expansion from even $l$ to odd $l$ does not
increase the rank of the flux operator. Put another way, odd spherical
harmonic modes with $l > 1$ are always in the null space of the light curve.
We will return to this point below.

\subsection{Decomposition of the flux operator}
%
\dfm{I think it would be good to move much of this section to the appendix. I don't think that the full derivation adds much, but obviously the definition of preimage and nullspace are useful. Perhaps we could just say that we derive these projections using SVD (see appendix for details) and then qualitatively define what they do?}

To better understand the properties of the null space of the light curve
mapping problem, let
%
us use singular value decomposition (SVD) to express
the flux operator as
%
\begin{align}
    \mathbf{A} = \mathbf{U} \, \mathbf{S} \, \mathbf{V}^\top
    \quad,
\end{align}
%
where $\mathbf{U}$ is a $(K \times K)$ orthogonal matrix,
$\mathbf{V}$ is a $(N \times N)$ orthogonal matrix,
and $\mathbf{S}$ is a $(K \times N)$ diagonal matrix.
%
The columns of $\mathbf{U}$ and the rows of $\mathbf{V}^\top$ are the left and right
\emph{singular vectors} of $\mathbf{A}$, and the entries along the
diagonal of $\mathbf{S}$ are the corresponding \emph{singular values}. If $\mathbf{A}$
has rank $R$, the first $R$ singular values will be nonzero, while the
remaining $N - R$ will be identically zero.
If we assume for definitess that $K > N$ (i.e., we have more flux observations
than surface map coefficients we're trying to constrain), we can express the
SVD matrices as
%
\begin{align}
    \mathbf{U}
     & =
    \left(
    \begin{array}{ccc|ccc}
            \mathbf{U}_{0,0}          & \cdots & \mathbf{U}_{0,R\text{-}1}          & \mathbf{U}_{0,R}          & \cdots & \mathbf{U}_{0,K\text{-}1}          \\
            \vdots                    & \cdots & \vdots                             & \vdots                    & \cdots & \vdots                             \\
            \mathbf{U}_{K\text{-}1,0} & \cdots & \mathbf{U}_{K\text{-}1,R\text{-}1} & \mathbf{U}_{K\text{-}1,R} & \cdots & \mathbf{U}_{K\text{-}1,K\text{-}1}
        \end{array}
    \right)
    \equiv
    \left(
    \begin{array}{c|c}
            \mathbf{U}_\bullet & \mathbf{U}_\circ
        \end{array}
    \right)
    %
    \\[1.5em]
    %
    \label{eq:S}
    \mathbf{S}
     & =
    \left(
    \begin{array}{ccc|ccc}
            \mathbf{S}_{0,0}\phantom{y} &                                     &                                    &                             &                                     &                                    \\
                                        & \ddots                              &                                    &                             & \mbox{\normalfont\Large\bfseries 0} &                                    \\
                                        &                                     & \mathbf{S}_{R\text{-}1,R\text{-}1} &                             &                                     &                                    \\
            \hline
                                        &                                     &                                    & \mathbf{S}_{R,R}\phantom{y} &                                     &                                    \\
                                        & \mbox{\normalfont\Large\bfseries 0} &                                    &                             & \ddots                              &                                    \\
                                        &                                     &                                    &                             &                                     & \mathbf{S}_{N\text{-}1,N\text{-}1} \\
                                        &                                     &                                    &                             &                                     &                                    \\
                                        & \mbox{\normalfont\Large\bfseries 0} &                                    &                             & \mbox{\normalfont\Large\bfseries 0} &                                    \\
                                        &                                     &                                    &                             &                                     &
        \end{array}
    \right)
    \equiv
    \left(
    \begin{array}{c|c}
            \mathbf{S}_\bullet & \mathbf{0}       \\
            \hline
            \mathbf{0}         & \mathbf{S}_\circ
        \end{array}
    \right)
    %
    \\[1.5em]
    %
    \mathbf{V}^\top
     & =
    \left(
    \begin{array}{cccccc}
            \mathbf{V}_{0,0}^\top          & \cdots & \mathbf{V}_{0,N\text{-}1}^\top          \\
            \vdots                         & \cdots & \vdots                                  \\
            \mathbf{V}_{R\text{-}1,0}^\top & \cdots & \mathbf{V}_{R\text{-}1,N\text{-}1}^\top \\[0.5em]
            \hline                                                                            \\[-0.85em]
            \mathbf{V}_{R,0}^\top          & \cdots & \mathbf{V}_{R,N\text{-}1}^\top          \\
            \vdots                         & \cdots & \vdots                                  \\
            \mathbf{V}_{N\text{-}1,0}^\top & \cdots & \mathbf{V}_{N\text{-}1,N\text{-}1}^\top
        \end{array}
    \right)
    \equiv
    \left(
    \begin{array}{cc}
            \mathbf{V}_\bullet^\top \\
            \hline
            \mathbf{V}_\circ^\top
        \end{array}
    \right)
\end{align}
%
where
$\mathbf{U}_\bullet$ is $(K \times R)$,
$\mathbf{U}_\circ$ is $(K \times K - R)$,
$\mathbf{S}_\bullet$ is $(R \times R)$,
$\mathbf{S}_\circ$ is $(K - R \times N - R)$,
$\mathbf{V}_\bullet^\top$ is $(R \times N)$,
and
$\mathbf{V}_\circ^\top$ is $(N - R \times N)$.
%
We may then express the decomposition of $\mathbf{A}$ as
%
\begin{align}
    \label{eq:A}
    \mathbf{A} =
    \left(
    \begin{array}{c|c}
            \mathbf{U}_\bullet & \mathbf{U}_\circ
        \end{array}
    \right)
    \left(
    \begin{array}{c|c}
            \mathbf{S}_\bullet & \mathbf{0}       \\
            \hline
            \mathbf{0}         & \mathbf{S}_\circ
        \end{array}
    \right)
    \left(
    \begin{array}{cc}
            \mathbf{V}_\bullet^\top \\
            \hline
            \mathbf{V}_\circ^\top
        \end{array}
    \right)
    \quad.
\end{align}
%
Inserting this into Equation~(\ref{eq:fAy}), we have
%
\begin{proof}{test_decomposition}
    \label{eq:ydecomp}
    \mathbf{f} & = \mathbf{A} \, \mathbf{y}
    \nonumber                               \\
    & =
    \left(
    \begin{array}{c|c}
            \mathbf{U}_\bullet & \mathbf{U}_\circ
        \end{array}
    \right)
    \left(
    \begin{array}{c|c}
            \mathbf{S}_\bullet & \mathbf{0}       \\
            \hline
            \mathbf{0}         & \mathbf{S}_\circ
        \end{array}
    \right)
    \left(
    \begin{array}{cc}
            \mathbf{V}_\bullet^\top \\
            \hline
            \mathbf{V}_\circ^\top
        \end{array}
    \right) \mathbf{y}
    \nonumber                               \\[0.5em]
    & =
    \mathbf{U}_\bullet \, \mathbf{S}_\bullet \, \mathbf{V}_\bullet^\top \, \mathbf{y}
    +
    \mathbf{U}_\circ \, \mathbf{S}_\circ \, \mathbf{V}_\circ^\top \, \mathbf{y}
    \nonumber                               \\[0.5em]
    & =
    \mathbf{U}_\bullet \, \mathbf{S}_\bullet (\,\mathbf{I}\,) \mathbf{V}_\bullet^\top \, \mathbf{y}
    +
    \mathbf{U}_\circ \, \mathbf{S}_\circ (\,\mathbf{I}\,) \mathbf{V}_\circ^\top \, \mathbf{y}
    \nonumber                               \\[0.5em]
    & =
    \mathbf{U}_\bullet \, \mathbf{S}_\bullet (\mathbf{V}_\bullet^\top \mathbf{V}_\bullet) \mathbf{V}_\bullet^\top \, \mathbf{y}
    +
    \mathbf{U}_\circ \, \mathbf{S}_\circ (\mathbf{V}_\circ^\top \mathbf{V}_\circ) \mathbf{V}_\circ^\top \, \mathbf{y}
    \nonumber                               \\[0.5em]
    & =
    (\mathbf{U}_\bullet \, \mathbf{S}_\bullet \, \mathbf{V}_\bullet^\top) \mathbf{V}_\bullet \mathbf{V}_\bullet^\top \, \mathbf{y}
    +
    (\mathbf{U}_\circ \, \mathbf{S}_\circ \, \mathbf{V}_\circ^\top) \mathbf{V}_\circ \mathbf{V}_\circ^\top \, \mathbf{y}
    \nonumber                               \\[0.5em]
    & =
    (\mathbf{U}_\bullet \, \mathbf{S}_\bullet \, \mathbf{V}_\bullet^\top) \, \mathbf{y}_\bullet
    +
    (\mathbf{U}_\circ \, \mathbf{S}_\circ \, \mathbf{V}_\circ^\top) \, \mathbf{y}_\circ
\end{proof}
%
where we defined
%
\begin{proof}{test_PN}
    \label{eq:yrow}
    \mathbf{y}_\bullet & \equiv \mathbf{V}_\bullet \mathbf{V}_\bullet^\top \mathbf{y}
    \\
    \label{eq:ynull}
    \mathbf{y}_\circ   & \equiv \mathbf{V}_\circ \mathbf{V}_\circ^\top \mathbf{y}
    \quad,
\end{proof}
%
and we used the fact that since $\mathbf{V}^\top$ is orthogonal,
%
\begin{proof}{test_orthogonality}
    \mathbf{V}_\bullet^\top \mathbf{V}_\bullet & = \mathbf{I}
    \nonumber                                                 \\
    \mathbf{V}_\circ^\top \mathbf{V}_\circ     & = \mathbf{I}
    \quad,
\end{proof}
%
where $\mathbf{I}$ is the identity matrix.
%
Now, recalling that $R$ is the number of nonzero singular values in
$\mathbf{S}$, it is evident from Equation~(\ref{eq:S}) that
%
\begin{align}
    \label{eq:S0}
    \mathbf{S}_\circ = \mathbf{0}
    \quad.
\end{align}
%
Therefore we may write Equation~(\ref{eq:ydecomp}) as
%
\begin{proof}{test_decomposition}
    \boxed{
        \mathbf{f} = \mathbf{A} \, \mathbf{y}_\bullet
        +
        \mathbf{0} \, \mathbf{y}_\circ
    }
\end{proof}
%
where the fact that $\mathbf{U}_\bullet \, \mathbf{S}_\bullet \, \mathbf{V}_\bullet^\top = \mathbf{A}$
follows directly from Equations~(\ref{eq:A}) and (\ref{eq:S0}).

So what does this all mean? By performing SVD, we separated
the surface map representation $\mathbf{y}$ into orthogonal components
$\mathbf{y}_\bullet$ and $\mathbf{y}_\circ$, consisting of the
terms that contribute to the light curve $\mathbf{f}$ and those
that don't, respectively. In particular, it is useful to consider
the linear operators we used to perform this decomposition
in Equations~(\ref{eq:yrow}) and (\ref{eq:ynull}):
%
\begin{proof}{test_PN}
    \mathbf{P} & \equiv \mathbf{V}_\bullet \mathbf{V}_\bullet^\top
    \\
    \mathbf{N} & \equiv \mathbf{V}_\circ \mathbf{V}_\circ^\top
    \quad,
\end{proof}
%
which we will call the \emph{preimage operator} and \emph{null space
    operator}, respectively. The preimage operator $\mathbf{P}$
transforms a vector $\mathbf{y}$ in the surface map basis in such a way
that it preserves the information in $\mathbf{y}$ that gets mapped
onto the light curve $\mathbf{f}$ via $\mathbf{A}$ (the \emph{preimage}) and discards the rest. The
null space operator $\mathbf{N}$ does the opposite: it preserves only
the information in $\mathbf{y}$ that gets mapped onto the zero
vector via $\mathbf{A}$ (the \emph{null space}).
%
In other words,
the $\mathbf{P}$ and $\mathbf{N}$ operators reveal the
components of the surface map $\mathbf{y}_\bullet$ that contribute to the
light curve and the components $\mathbf{y}_\circ$ that don't.
Likewise, $\mathbf{y}_\bullet$ represents all the information that can be
learned from a stellar light curve, and $\mathbf{y}_\circ$ represents all the
information that cannot.

\begin{figure}[p!]
    \begin{centering}
        %
        \textbf{Single light curve at $I = 60^\circ$}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_60a.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_60b.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_60c.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_60d.pdf}
        \oscaption{nullspace_preimage}{%
            Decomposition of a surface map (left column) into its
            preimage (center) and null space (right) components
            for different surfaces, and their corresponding contributions
            to the rotational light curve. The preimage is the
            set of surfaces modes that map onto the light curve;
            the null space is the set of modes that do not.
            An inclination of $60^\circ$ is assumed in all plots.
            The vast majority of surface modes are in the null space
            of the light curve problem and therefore do not contribute
            to the observed flux.
            \label{fig:nullspace_preimage}
        }
    \end{centering}
\end{figure}

\begin{figure}[p!]
    \begin{centering}
        %
        \textbf{Single light curve at $I = 85^\circ$}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_85a.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_85b.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_85c.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_85d.pdf}
        \oscaption{nullspace_preimage}{%
            Same as Figure~\ref{fig:nullspace_preimage}, but for
            a stellar inclination of $85^\circ$. As the stellar rotation vector
            becomes perpendicular to the line of sight, it becomes
            more difficult to constrain latitudinal information.
            \label{fig:nullspace_preimage_85}
        }
    \end{centering}
\end{figure}

It is instructive to visualize these components in
an actual surface mapping exercise. Figure~\ref{fig:nullspace_preimage}
shows the decomposition of four hypothetical surfaces (left
column) into preimage (center) and null space (right)
components under the flux operator $\mathbf{A}$, which we compute for
definiteness at an inclination of $I = 60^\circ$ over a full rotation.
The light curves corresponding to each of these surfaces are also shown.
Note that both the true map and its associated light curve are simply equal to
the sum of the preimage and null space components.%
\footnote{%
    This follows from the orthogonality of $\mathbf{V}$:
    $
        \mathbf{V} \mathbf{V}^\top =
        \mathbf{I} =
        \mathbf{V}_\bullet \mathbf{V}_\bullet^\top  + \mathbf{V}_\circ \mathbf{V}_\circ^\top =
        \mathbf{P} + \mathbf{N}
    $, so $\mathbf{y} = \mathbf{y}_\bullet + \mathbf{y}_\circ$.
}
%
As expected, all of the information in the light curve comes
from the preimage, and the null space contributes exactly zero
flux at all phases. However, most of the information about the
\emph{surface} is stuck in the null space!

In the top row of the figure, corresponding to a surface with
a single large spot, it is clear that the light curve
contains information about the presence of the spot at roughly the
correct longitude and latitude. There are additional artefacts
across the stellar surface, and the exact shape of the spot is
not well constrained by the data; these issues, however, can
easily be resolved with a circular spot prior.

The degeneracies of the mapping problem are much more apparent
in the second and third rows, corresponding to surfaces with
many, smaller spots. The locations, sizes, and the very existence
of most of the spots are simply not encoded in the light curve.
Even with an extremely restrictive prior, it may be difficult---if
not impossible---to learn the properties of the spots
from these light curves.

The final row corresponds to a surface with much higher resolution
features; this example highlights how the information content of
light curves all but vanishes at small scales. Virtually all of the
spatial information at the scales of interest is in the null space.

\subsection{Implications for inference}
%
It is useful to consider Figure~\ref{fig:nullspace_preimage} in the
context of an inference exercise. One can equivalently think of
the preimage as the solution to a least squares inference problem
in which a completely uninformative prior%
\footnote{%
    More specifically, a zero-mean, infinite variance Gaussian prior on the coefficients of $\mathbf{y}$.
}
is placed on the spherical harmonic
coefficients. In this case, the least squares solution is given by
%
\begin{proof}{test_svd_lstsq}
    \label{eq:inference}
    \hat{\mathbf{y}}
    &=
    \lim_{\lambda \rightarrow 0} \left( \mathbf{A}^\top \mathbf{A} + \lambda \mathbf{I}\right)^{-1} \mathbf{A}^\top \mathbf{f}
    \nonumber\\
    &= \mathbf{y}_\bullet
    \quad.
\end{proof}
%
In this sense, $\mathbf{y}_\bullet$ represents our knowledge about the
surface of a star after an observation if we have no prior information
whatsoever on $\mathbf{y}$.
\emph{Any other information about the surface must come from the prior.}%

Equation~(\ref{eq:inference}) is the solution to the surface map
we would obtain if we knew \emph{nothing} about the stellar surface
before analyzing the light curve. In practice, this is never really
the case. For instance, we know that stellar surfaces
must have non-negative intensities everywhere. While this may seem
like a trivial prior, non-negativity can be a powerful degeneracy-breaking
constraint \citep[e.g.,][]{Fienup1982}. Moreover, we know that
stellar surfaces usually consist of localized features; under an
appropriate compactness prior, solutions
like the preimage in the bottom row of
Figure~\ref{fig:nullspace_preimage} (for example) could be
confidently ruled out.

The effect of different priors on the structure and size of the null
space is beyond the scope of this paper. In general, this is a particularly
difficult question to address because non-Gaussian priors on the
surface map break the linearity of the problem. While compactness priors
(like the assumption of a small number of discrete circular spots) can
break many of the degeneracies discussed here, simulation-based
arguments show that it is still not possible to uniquely constrain
their number, locations, or sizes from individual light curves
\citep{Basri2020}.

% TODO:
% In principle, the decomposition into preimage and nullspace could
% depend on the basis we're using to represent the surface map. If
% I do this in the pixel basis, I typically get a different result,
% BUT the result converges to the plots in Figure 4 as the number of
% pixels in the basis increases!
% So I suspect that as long as the basis is complete and has certain
% properties -- orthogonality perhaps? -- the preimage/nullspace
% decomposition is unique.
% Another interesting thing is that the preimage/nullspace is the SAME
% even when the "true" map is generated at much higher spherical
% harmonic degree. So the thing I'm showing seems to actually be
% robust to all these things!
% Perhaps Hogg has some insight?

\subsection{Dependence on inclination}

Figure~\ref{fig:nullspace_preimage_85} shows the same decomposition
of stellar surfaces into what can be learned (the preimage) and what
cannot be learned (the null space) from a light cuve, but this time
for stars viewed at an inclination $I = 85^\circ$. Interestingly,
the structure of the null space is somewhat different; in particular,
features in the preimage are latitudinally smeared. This
issue is well known \citep[e.g.,][]{Cowan2009,Basri2020}, and any
information about which hemisphere a feature is in formally vanishes
as $I \rightarrow 90^\circ$.

Instead of lamenting the difficulties of constraining stellar surface features
at near-edge-on orientations, let us focus on the fact that the
null space is a function of inclination.
%
A useful property of
the spherical harmonics is that a rotation operation on any
component in the basis can only change the order $m$ of the
harmonic; the degree $l$ is constant under rotation. In other
words, rotating any of the spherical harmonics in Figure~\ref{fig:ylms}
about an arbitrary axis simply yields a weighted sum of the spherical
harmonics along its row.%
\footnote{For instance, rotation of $Y_{1,-1}$ by $90^\circ$ about
    $\hat{\mathbf{x}}$ yields $Y_{1,1}$; other rotations will in general
    yield a weighted combination of $Y_{1,-1}$, $Y_{1,0}$, and $Y_{1,1}$.}
This means that changing the inclination of the star---which changes
the axis of rotation in the observer's frame---simply changes the weighting
of modes that give rise to certain signals in the light curve.
This, in turn, results in the dependence of the null space on inclination.

To better understand this effect, let us
define the \emph{posterior shrinkage} $S$ as
%
\begin{align}
    \label{eq:shrinkage}
    S \equiv 1 - \lim\limits_{\sigma_0^2 \rightarrow \infty}
    \frac{\sigma^2}{\sigma_0^2}
    \quad,
\end{align}
%
where $\sigma_0^2$ is the prior variance
and $\sigma^2$ is the posterior variance
on a particular surface mode we are trying to constrain
\citep[see, e.g.,][]{Betancourt2018}.
The posterior shrinkage is a measure of how informative a measurement is about a
given mode on the surface in the limit of infinite signal-to-noise ratio (SNR)
and is independent of what the stellar surface actually looks like.
If, at infinite SNR and with a completely uninformative prior,
a particular mode can be learned exactly from a dataset, the posterior
shrinkage is defined to be unity. Conversely, if the data is completely
unconstraining of that mode (i.e., it is entirely in the null space),
$S$ will tend to zero.

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/nullspace_ensemble_single.pdf}
        \oscaption{nullspace_ensemble_single}{%
            Posterior shrinkage as a function of spherical harmonic
            degree $l$ given a single observation of a star at a random
            inclination (thin blue curves). The mean shrinkage is shown
            as the thicker curve. The information content of light curves
            tends to zero as $l$ increases, and odd $l > 1$ modes are in the
            null space at all inclinations.
            \label{fig:nullspace_ensemble_single}
        }
    \end{centering}
\end{figure}

Figure~\ref{fig:nullspace_ensemble_single} shows the posterior shrinkage
given a single observation of a star at a random inclination.
Each thin blue curve corresponds to a particular draw from an isotropic
inclination distribution; the thick blue curve is the average over 300 trials.
For some of the low-degree modes, the shrinkage is relatively high: it is
fairly easy to constrain the dipole moment from a light curve, as this is
usually the dominant sinusoidal signal. However, as the degree $l$
increases, the shrinkage decreases dramatically: at $l = 14$, corresponding
to features on scales of roughly $13^\circ$, the light curve
can only tell us about $\sim 10\%$ of the total information about what the
surface looks like. As $l$ increases further, $S$ tends to zero.
Another important feature of the shrinkage, which we hinted at above,
is that it is exactly zero for
all odd-degree modes above $l = 1$. This is a well-known fact: all odd spherical
harmonics other than the dipole are in the null space \emph{regardless of
    inclination} \citep[e.g.,][]{Luger2019}. In other words, these spherical
harmonics are perfectly antisymmetric in projection over the unit disk
when viewed from any orientation. Absent structure to break these symmetries
(see below), we simply cannot learn anything about these modes from
stellar light curves. If we average over the shrinkage for all modes up to $l=15$,
we find that a single light curve measurement can only tell us $\sim 9\%$
of the information about the surface on those scales.

Fortunately, however, there is quite a bit of variance in the shrinkage
for different values of the stellar inclination.
As we will see, we can use this dependence of the null space on inclination to our
advantage. If we could observe a star from many different vantage points,
we would be able to break many of the degeneracies at play, since we
would get different constraints on the amplitude of each mode when viewed
at different inclinations. This, of course, is not possible (at least not
yet!). But what we \emph{can} do is observe many similar stars, each viewed
at a different (random) inclination, and attempt to learn something about
the properties of the ensemble of stars as a whole.
In the
following section, we explore the role of ensemble analyses in breaking
the degeneracies of the mapping problem in more detail.


\subsection{Ensemble analyses}
\label{sec:ensemble}

In an ensemble analysis, we assume we observe the lightcurves of many stars
that are ``similar'' in some statistical sense. As a thought experiment,
let us consider an extreme version of ensemble analysis in which all the
stars in our sample happen to have \emph{identical} surfaces. We will
still assume they are oriented at random inclinations, as we would expect for
field stars.
%
Figure~\ref{fig:nullspace_ensemble} shows the posterior shrinkage
curves for this hypothetical scenario, assuming we have access to
light curves of 1 (blue), 3 (orange), 10 (green) and 30 (red)
identical stars viewed at random inclinations.

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/nullspace_ensemble.pdf}
        \oscaption{nullspace_ensemble}{%
            Similar to Figure~\ref{fig:nullspace_ensemble_single}, but
            assuming the observer can measure the light curves
            of 1 (blue), 3 (orange), 10 (green), and 30 (red) \emph{identical} stars
            viewed at random orientations.
            As we saw in Figure~\ref{fig:nullspace_ensemble_single},
            the information content in the light curve of a star observed
            from a single vantage point approaches zero as $l$
            increases. However, observing many identical stars from different vantage points
            allows one to recover nearly all of the information in the
            even spherical harmonic modes. This is why ensemble analyses of
            many similar stars at different inclinations allows us to infer
            their surface properties.
            \label{fig:nullspace_ensemble}
        }
    \end{centering}
\end{figure}

The addition of light curve measurements at different orientations
increases the posterior shrinkage at all even spherical harmonic degrees
(the odd degrees, as we mentioned above, are always invisible).
Note that since we are in the limit of infinite SNR, the fact that we have
more light curves (i.e., more data) is irrelevant: the
increase in the shrinkage is instead due to the fact that our observations
from different vantage points broke some degeneracies in the problem.
This is a consequence of the fact we mentioned in the previous section:
the null space (for the even modes)
is a strong function of the inclination.

If we average over all modes, we obtain a total shrinkage of $\sim 24\%$
for $l\leq15$ when our sample size is 3 (orange curves): we have more than doubled the
information content of our observations. If we further increase our sample size
to 10 (green curves), the posterior shrinkage approaches $100\%$ for all even $l\leq15$
modes, effectively saturating for a sample size of 30 (red curves).
Thus, if we were able to measure light curves of identical stars
from many different inclinations, the null space would consist \emph{only}
of the odd modes. In the limit of a large number of light curves,
and assuming all stars in the sample have identical surfaces,
\textbf{ensemble analyses can tell us up to
    half of all the information about the stellar surfaces.}

\begin{figure}[p!]
    \begin{centering}
        %
        \textbf{Ensemble of light curves at many inclinations}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_a.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_b.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_c.pdf}
        \\[1em]
        \includegraphics[width=\linewidth]{figures/nullspace_preimage_d.pdf}
        \oscaption{nullspace_preimage_ensemble}{%
            Same as Figures~\ref{fig:nullspace_preimage} and \ref{fig:nullspace_preimage_85}, \
            but assuming we can measure the light curves of these stars from many
            different inclinations. In this limit, the information content of our
            data approaches $50\%$ of the spatial information about the surface.
            \label{fig:nullspace_preimage_ensemble}
        }
    \end{centering}
\end{figure}

To understand what we can learn about the surfaces in this limit, let us
return to the stellar surfaces we considered in Figures~\ref{fig:nullspace_preimage}
and \ref{fig:nullspace_preimage_85}.
Figure~\ref{fig:nullspace_preimage_ensemble} shows the same decomposition of
these surfaces into preimage and null space, but this time assuming we measure
the light curves of these stars from many different random inclinations.
As expected, the partition of information between the preimage and the nullspace
is about $50{-}50$.
Because spots are compact features, they
are necessarily made up of a continuum of spherical harmonic modes spanning
many different values of $l$: they can therefore be seen in both the even
modes (the preimage) and the odd modes (the null space). The absence of
information about the odd modes therefore does not affect our ability to
infer the shape and location of the features on the surface.%
\footnote{
    There is even some hope of deciphering complex alien messages
    (last row in the figure) in this limit!
}
Interestingly, however,
the symmetries at play require spots to be paired with antipodal dark mirror
images in the preimage, and with \emph{bright} ones in the null space
(which sum to perfectly cancel out in the true map). Thus, there are still
degeneracies in the ensemble problem, but they are much easier to break
with a suitable choice of prior. For instance, in the single spot case
(top row), the ``ghost'' image in the southern hemisphere is surrounded by
a bright ring (whose effect is to cancel out its contribution to the flux);
either a compactness prior or a prior that enforces uniformity in the
background could easily penalize that feature in the fit. This may be
much harder to do for surfaces like that shown in the second row, but
we can still (in principle) learn about the size, shape, and latitude
(if not the number) of starspots from the light curves.

There are several practical reasons why this kind of ensemble analysis
may be very difficult (but not impossible!) in practice. Before
we address those, let us consider an important effect that we have not
addressed thus far in this paper: limb darkening.

\subsection{The effect of limb darkening}
\label{sec:limbdark}

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/nullspace_ensemble_ld.pdf}
        \oscaption{nullspace_ensemble_ld}{%
            Same as Figure~\ref{fig:nullspace_ensemble},
            but for limb-darkened
            stars with quadratic coefficients $u_1 = 0.5$ and $u_2 = 0.25$. Odd
            modes can now be probed, at the expense of the even modes.
            \label{fig:nullspace_ensemble_ld}
        }
    \end{centering}
\end{figure}

Typically, the shallower the angle between the line of sight
and the stellar surface normal, the higher up in the stellar
atmosphere optical depth unity is reached. At optical wavelengths,
lines of sight directed toward the limb of the star therefore
probe cooler temperatures, resulting in the well-known effect of
\emph{limb darkening}. Features close to the limb of the star therefore
contribute less to the total outgoing flux, and this must be
accounted for when computing the effect of a rotating starspot
on the light curve. Limb darkening is often parametrized
as a low-order polynomial in the cosine of the line of sight angle
\citep{Kopal1950}.

To understand how limb darkening affects our ability to infer
surface properties from stellar light curves,
let us repeat the experiment from the previous section,
this time with moderate quadratic limb darkening (with coefficients
$u_1 = 0.5$ and $u_2 = 0.25$, \dfm{although our conclusions do not qualitatively change for different values}).
The top panel of Figure~\ref{fig:nullspace_ensemble_ld} shows the
posterior shrinkage plot (same as Figure~\ref{fig:nullspace_ensemble}, but
this time accounting for limb darkening). Interestingly, there is no longer a clean division of
the null space between even and odd modes in the limit of a large number
of light curves.
This is because limb darkening effectively lifts odd modes out of the null space, \emph{at the
    expense of the even modes}. While no coefficient lies entirely in the
null space ($S = 0$) when limb darkening is present, no coefficient
can be uniquely inferred ($S = 1$), either. This can be understood by noting that
a polynomial limb darkening law can be written
exactly as a linear combination of the $m=0$ spherical harmonics
up to a degree equal to the order of the limb darkening
\citep[in this case, $l = 2$;]{Luger2019,Agol2020}.
Since the limb darkening operation is a (multiplicative) downweighting of
the surface intensity, the map seen by the observer is just the product
of the spherical harmonic representation of the surface ($\mathbf{y}$)
and the spherical harmonic representation of the limb darkening profile.
And since spherical harmonics are just polynomials on the surface of the sphere,
the product of spherical harmonics of degree $l_1$ and $l_2$ is a spherical
harmonic of degree $l_1 + l_2$. This means that the linear limb darkening
component ($l = 1$) effectively raises the degree of all spherical harmonic
coefficients of the surface map by one. This has the effect of reversing
the null space: under \emph{only} linear limb darkening, it is the \emph{even}
modes that would be in the null space. However, the quadratic
limb darkening term ($l = 2$) raises the degree of all spherical harmonics by two,
so its presence ensures that the even modes can still be probed to some extent.
In reality, the true limb darkening profile of a stellar surface is more
complicated than a two-parameter quadratic model can capture; but one may still
expand it as an arbitrary order polynomial, in which case the argument still
applies---limb darkening mixes the null space and the preimage in a nontrivial
way.
%
The fact that no coefficient can be determined uniquely---i.e., there are
perfect degeneracies involving \emph{all} modes on the surface---could make it more
difficult in practice to perform ensemble analyses on limb-darkened stars.

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/nullspace_ensemble_ld_var.pdf}
        \oscaption{nullspace_ensemble_ld_var}{%
            Same as Figure~\ref{fig:nullspace_ensemble_ld}, but allowing for
            a 10\% variation in the limb darkening coefficients $u_1$ and $u_2$
            across different observations. For $\gtrsim 30$ light curves,
            there is virtually no null space up to at least $l_\mathrm{max} = 15$.
            \label{fig:nullspace_ensemble_ld_var}
        }
    \end{centering}
\end{figure}

In reality, it is unlikely that all stars in a given ensemble will have exactly
the same limb darkening coefficients, however ``similar'' the stars may be.
Figure~\ref{fig:nullspace_ensemble_ld_var} shows the same
posterior shrinkage plot as Figure~\ref{fig:nullspace_ensemble_ld},
but for the case where each star has coefficients
$u_1 = 0.5 \pm 0.05$ and $u_2 = 0.25 \pm 0.025$; i.e., we add a scatter of
10\% in the value of these coefficients. The plot shows the posterior
shrinkage in the hypothetical case where we know the coefficients for
each star exactly. Now, as the size of the ensemble increases, the posterior
shrinkage approaches unity \emph{for all spherical harmonic modes}.
In the same way that the null space is a strong function of the inclination,
allowing us to chip away at it with observations at different inclinations,
the null space is also a strong function of the limb darkening law. Even a
small amount of variance in the coefficients is sufficient to constrain all surface
modes exactly (in the limit of infinite SNR and a large number of light curves).

In practice, of course, we will never know the limb darkening coefficients
exactly. In the next section, we will revisit this and other assumptions we
made above in a more sober light.

\subsection{A reality check}
\label{sec:reality-check}

There are three major points that make the kind of ensemble
analyses discussed above difficult in practice. First, and perhaps
most obviously, stars are
not identical, no matter how ``similar'' we think they may be.
Even stars of the same spectral type, age, and metallicity will
in general have different configurations of spots on their surfaces.
When we perform an ensemble analysis on the light curves of a
heterogeneous group of stars, we are learning something about the
\emph{distribution} of surface properties across all the stars in
the sample---not the surface properties of any individual star.
What exactly we can learn in this case is not immediately obvious,
and requires a detailed investigation. This is the subject of
the next paper in this series \citepalias{PaperII}, where we show that,
armed with a good model,
we can learn \emph{a lot} about the distribution of starspot
properties of a heterogenous group of stellar light curves.

The second point is that while observing many stars at different
inclinations breaks many of the degeneracies at play, our analysis
above assumed we knew what the values of the inclinations were.
In practice, this will usually not be the case. While we may have
good priors for some stars (from spectroscopic $v\sin i$ measurements,
or from the assumption that transiting exoplanet hosts are likely
to have inclinations close to $I=90^\circ$), for the vast majority
of field stars we won't know much a priori. Since the inclination
is typically degenerate with the spot latitude \citep[e.g.,][]{Walkowicz2013},
this decreases the constraining power of ensemble analyses.
However, as we show in \citetalias{PaperII}, there is still enough
information in a large ensemble to independently constrain the spot latitudes
and the \emph{individual} stellar inclinations.

The final point concerns limb darkening, which also has a strong
effect on the structure of the null space. While limb darkening can
help us by breaking degeneracies, in practice it is likely to be more
of a problem, since the use of incorrect limb darkening coefficients can lead
to bias in the spot properties when doing inference.
It is therefore extremely important to use reliable limb darkening models
when doing ensemble analyses; we also explore this in \citetalias{PaperII}.

\section{The baseline problem}
\label{sec:baseline}

\subsection{A fundamental issue of units}

Thus far we have focused our discussion on theoretical aspects concerning
what can and cannot be learned from disk-integrated photometric measurements
of stellar surfaces. In this section, we discuss an important degeneracy
introduced by how we actually measure stellar light curves, which we
will refer to as the \emph{baseline degeneracy}.

To understand this degeneracy, consider how we might go about simulating
a stellar surface.
%
We might add a dark spot somewhere on the surface, either by expanding
it in spherical harmonics or by gridding up the stellar surface and
setting the intensity of pixels within the spot to a low value. To compute
the light curve, we integrate over the
projected disk at each point in time.
% , either analytically (if we're
% in the spherical harmonic basis) or numerically by computing a weighted
% sum of the visible pixels (if we're in the pixel basis).
The resulting
light curve will have strange units, so we might then divide by the
integral of the background intensity over the unit disk, so that we are
now in what we will call \emph{fractional units}: the flux as a
fraction of the flux we would measure if the star had no spots on it.

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/mean_normalization.pdf}
        \oscaption{mean_normalization}{%
            An example of the baseline problem.
            \emph{Top:} Consider a star with a single
            equatorial spot of contrast $c$ viewed at a certain inclination.
            The total flux (in some units) as a function of time is shown as
            the blue curve. Now, consider a second star,
            identical in all respects to the first, except that (1) the equatorial spot
            has half the contrast (i.e., $\nicefrac{c}{2}$); and (2) there is
            a second, large spot centered on the pole. The corresponding light
            curve is shown as the dashed orange curve.
            The orange light curve is different from the blue one in two ways:
            (1) since the equatorial spot has half the contrast, the amplitude of the associated
            dips in the light curve is half that of the first star; and
            (2) since the polar spot is azimuthally symmetric, its only
            contribution is a net darkening at all phases.
            \emph{Bottom:} The true baseline level of a stellar
            light curve, which corresponds to the flux one would measure in the
            absence of any spots, is almost always unknown. Photometric measurements
            are therefore meaningful only in a relative sense, i.e., as deviations from
            the mean, median, or maximum level of the light curve.
            The bottom panel shows the same two light curves, this time plotted as
            deviations in parts per thousand (ppt) from their respective maxima.
            To the observer, the two light curves are \emph{indistinguishable}.
            In the absence of baseline information, there
            exists a perfect degeneracy between the total spot coverage
            and the contrast of any individual feature on the surface.
            As a consequence, the total spot coverage of a star cannot be
            inferred from single-band photometry.
            \label{fig:mean_normalization}
        }
    \end{centering}
\end{figure}

The top panel of Figure~\ref{fig:mean_normalization} shows two mock
light curves we might compute following the procedure above. The solid blue
curve corresponds to the light curve of a star with a single large
equatorial spot of contrast $c$ viewed at an inclination $I = 60^\circ$.
The dashed orange curve corresponds to a star with a spot at the same
location but half the contrast, plus a large polar spot of comparable
contrast. Because the equatorial spot on this star has half the contrast of that on
the first star, the peak-to-trough amplitude of the orange light curve is half that of
the blue light curve. Moreover, since the polar spot is always in view on
this star, the peak flux is itself only about half that of the first star.
If we were given these two light curves \emph{in these fractional units},
we might be able to infer these basic differences between the two stars
(setting aside for the moment all the issues with the null space discussed
in the previous section).

However, \textbf{we do not observe stellar light curves in fractional units.}
Instead, we typically observe in units of counts on
the detector, which depend on the luminosity of the star, the distance to
the star, and various properties of the telescope. But even if we knew all these
things, we usually do not know the brightness of the unspotted
photosphere, the quantity we need in order to scale each observed light curve
to transform it into the fractional units of the top panel of the figure.

Usually, this isn't much of an issue. Astronomers typically circumvent
this by self-normalizing the data: i.e., dividing the flux by the mean,
median, maximum, or some similar statistic of the light curve.
%
This operation folds the unknowability of
the true units under the rug and transforms the light curve into a \emph{relative}
measurement of the star's temporal variability. While relative measurements
are typically what we are interested in anyways, this normalization procedure
can sometimes fool us into thinking we have access to information that
is simply not observable in single-band photometry. To understand why,
consider the lower panel of Figure~\ref{fig:mean_normalization}, which shows the
same two light curves in what we will call \emph{relative units}. To
get the light curves in these units, we followed the common procedure of
dividing each by the observed ``continuum'' level (the maximum flux in the
light curve), subtracting unity, and multiplying by $1{,}000$, yielding
relative fluxes in units of parts per thousand (ppt).

The two light curves, which were distinct in the fractional units we used
to generate them, are \textbf{indistinguishable in the relative units
    in which we observe them}. There is absolutely no information in the
relative light curves that can differentiate between the two stellar
surfaces shown in the figure. The depth of each of the dips cannot tell
us about either the contrast of individual spots or the total number of spots;
in fact, in single-band photometry, these two quantities are perfectly
degenerate with each other.

\subsection{Ensemble analyses don't help}
\label{sec:basic-gp}
%
To explore this point in a bit more detail, let us consider the baseline
degeneracy in the context of ensemble analyses. Even though we can't
uniquely infer contrasts or numbers of spots from individual light curves,
perhaps we could harness the power of the ensemble.
%
Let us therefore go
back to our thought experiment in which we added spots to a stellar
surface. Assuming for simplicity that all spots have the same contrast $c$,
every time we add a spot the flux (in fractional units)
decreases by an amount proportional to $c$, so to first order we can approximate an
individual light curve as
%
\begin{align}
    \mathbf{f}(c, n) = \mathbf{1} - c \sum_{i=0}^{n-1} \mathbf{g}(\pmb{\theta}_i)
\end{align}
%
where $\mathbf{g}(\pmb{\theta}_i)$ is some (complicated) function of the properties
of the $i^\mathrm{th}$ spot
as well as the stellar inclination, rotation period, etc., which we
denote by $\pmb{\theta}_i$.
Now, consider many stellar light curves drawn from some distribution
controlling the stellar and starspot properties with probability density
$p(\pmb{\theta})$.
%
The mean of the distribution of
light curves (still in fractional units) is then given by
%
\begin{align}
    \mu(c, n) & = \mathrm{E} \Big[ \mathbf{f} (c, n) \Big] \nonumber                                      \\
              & = 1 - c \, \mathrm{E} \bigg[ \sum_{i=0}^{n-1} \mathbf{g}(\pmb{\theta}_i) \bigg] \nonumber \\
              & = 1 - c \, n \, \alpha
    \quad,
\end{align}
%
where $\mathrm{E}\big[\cdots\big]$ denotes the expected value and
%
\begin{align}
    \alpha \equiv \int \mathbf{g} (\pmb{\theta}) p(\pmb{\theta}) \mathrm{d} \pmb{\theta}
\end{align}
%
is the expected value of $\mathbf{g}$.
%
Similarly, the variance of the distribution may be computed as
%
\begin{align}
    \sigma^2(c, n) & = \mathrm{Var} \Big[ \mathbf{f}(c, n) \Big] \nonumber                         \nonumber \\
                   & = \mathrm{Var} \Big[ c \, \sum_{i=0}^{n-1} \mathbf{g}(\pmb{\theta}_i) \Big]   \nonumber \\
                   & = c^2 \, \mathrm{Var} \Big[ \sum_{i=0}^{n-1} \mathbf{g}(\pmb{\theta}_i) \Big] \nonumber \\
                   & = c^2 \sum_{i=0}^{n-1} \mathrm{Var} \Big[  \mathbf{g}(\pmb{\theta}_i) \Big]   \nonumber \\
                   & = c^2 \, n \, \beta
    \quad,
\end{align}
%
where $\mathrm{Var}\big[\cdots\big]$ denotes the variance,
%
\begin{align}
    \beta \equiv \int \mathbf{g}^2 (\pmb{\theta}) p(\pmb{\theta}) \mathrm{d} \pmb{\theta}
\end{align}
%
is the expected value of $\mathbf{g}^2$, and we used the fact that the variance
of the sum of independent random variables is equal to the sum of their variances.

%

To summarize, the mean and variance of the ensemble of stellar light curves
in fractional units is
%
\begin{align}
    \mu      & = 1 - c \, n \, \alpha
    \nonumber                         \\
    \sigma^2 & = c^2 \, n \, \beta
    \quad,
\end{align}
%
for some complicated functions $\alpha$ and $\beta$ of the distribution of
stellar inclinations,
rotation periods, and starspot properties.
%
If our observations were collected in these fractional units,
we could uniquely infer the spot contrast $c$ and the number of spots $n$,
since these enter as $c \, n$ and $c^2 n$ in the expressions for the mean
and variance, respectively, and these are straightforward statistics to
compute from the ensemble.
%
However, because of observations are made in \emph{relative} units, in which
we typically normalize to the mean, photometry can only tell us about the \emph{ratio}
%
\begin{align}
    \frac{\sigma}{\mu}
     & \propto \frac{c \sqrt{n}}{1 - c n \alpha}
\end{align}
%
for some value of $\alpha$.
In other words, even photometric ensemble analyses cannot tell
us about the values of the contrast and the number of spots independently.

A direct consequence of this baseline degeneracy is that it is not in general
possible to constrain the total spot coverage of a star from
single-band photometry. Assuming no spot overlap, the total spot coverage $f_S$
is simply the (average) area of a spot
times the total number of spots divided by the total area of the sphere, which
may be expressed as
%
\begin{proof}{test_fS}
    \label{eq:fS}
    f_S = \frac{1}{2}\left(1 - \left<\cos r\right>\right)n
    \quad,
\end{proof}
%
given an angular spot radius $r$.
While $r$ may be constrained from the covariance structure of the data
\citepalias{PaperII}, $n$ cannot.

The arguments above are heuristic and based on only the first two moments
of the distribution of light curves in an ensemble.
It is possible, at least in principle, that higher order moments of the data
could encode information to
break the $c-N$ degeneracy, but these are in general much more difficult
to constrain from the data. And even if this were the case, the fact remains
that degeneracies like the polar spot effect are fundamental: recall
Figure~\ref{fig:mean_normalization}, in which the two stars
have identical relative light curves, but very different spot coverage
fractions.

\subsection{Effect on the covariance structure}
\label{sec:covariance}

There is one final subtle point concerning the baseline degeneracy that merits discussion.
The common procedure of normalizing light curves to their mean, median, or
maximum level does not only change the \emph{units} of the data: it changes
the very covariance structure of the light curves.

To understand why, let us consider the procedure of normalizing a light curve
to its mean value. Whenever we scale our data, we must always be sure to
scale the errorbars accordingly. Since in this case we are dividing the
flux by the mean, one might imagine that we could simply divide each of
the measurement uncertainties by the same amount. However,
this is technically incorrect!

\begin{figure}[t!]
    \begin{centering}
        \includegraphics[width=\linewidth]{figures/nonstationarity.pdf}
        \oscaption{nonstationarity}{%
            An example of why normalized light curves are non-stationary.
            The top panel shows $1{,}000$ samples from a unit-mean sinusoid with
            an amplitude of 10\% and a period of 10 days, much longer than the
            1 day observation baseline. The bottom panel shows the same light curves,
            each normalized to its own mean. Because the mean tends to be near
            the center of the observation window, points near $t=0.5$ are driven
            to values very close to unity, while points near the edges have much
            larger scatter.
            \label{fig:nonstationarity}
        }
    \end{centering}
\end{figure}

Consider the example in Figure~\ref{fig:nonstationarity}. The top panel
shows $1{,}000$ samples from a sinusoid with random phases and a period
equal to ten times the observational window.
%
In this limit, each light curve is approximately
linear, which causes its mean value to roughly coincide with the midpoint of the
observation window.
%
Division by the mean value (lower panel) results in points near the
midpoint being driven to unity and points near the edges (whose values differ
the most from the mean) to be driven to both large and small values.
%
If our error bars in the original data were uniform (homoscedastic),
the error bars in the normalized light curves are
not: the standard deviation (or variance) of the data is now
distinctly dependent on the phase.

While the example shown in the figure is fairly extreme, the idea here
is quite general: the normalization procedure changes the covariance
structure of the data. In most cases, the non-stationarity (i.e., the
phase dependence) of the variance will be quite small. The effect is primarily
important for light curves with periods much longer than
the observation window. In these cases, not accounting for this effect
could introduce bias in light curve analyses. A detailed investigation of
this effect is beyond the scope of this paper, but we do present a
method to correct the covariance matrix of normalized light curves for
this issue in \citetalias{PaperII}.

\section{Discussion and conclusions}

\xxx{STILL WORKING BELOW.}

\subsection{}

Fortunately, it may be possible to break these degeneracies (at least to some extent)
by collecting photometric data in multiple wavelength bands, an idea that has
been explored recently \citep[e.g.,][]{Gully2017,Guo2018}. Assuming the
locations and sizes of a star's spots are constant in wavelength, the amplitude
of the light curve in different bands (and in particular its slope as a function
of wavelength) can be used to directly constrain the
temperature, and hence the contrast, of the spots. This effectively breaks the
$c-N$ degeneracy. In practice, the effective size of spots may be different
at different wavelengths, which could complicate this picture somewhat. An extension
of the formalism in this paper to multi-band photometry, and a detailed investigation
of the information content of such observations, is left to future work.


\xxx{...}
%

In principle, the maximum level of a light curve could set a lower limit on
the location of the baseline. However, this applies only if the surface is
known to be made up of \emph{exclusively} dark spots. The presence of bright
spots (faculae), which are common on the Sun, make it effectively impossible
for one to infer the baseline from single-band photometry alone.
\citep{Basri2018}.

Recently, \citet{Morris2020} used \emph{Kepler}, \emph{K2}, and \emph{TESS}
light curves to derive a relationship between stellar age and spot
coverage. Because of the intractability of the marginal likelihood function,
that study used an approximate Bayesian computation (ABC) method to
infer spot properties from a large ensemble of stars. \citet{Morris2020}
developed a fast, approximate forward model for light curves of spotted stars
\citep[\textsf{fleck};][]{Morris2020b}, which they used to generate a large
number of prior samples for different values of the spot radii, contrasts,
and latitude distributions. For each collection of samples generated from a
given set of hyperparameters, \citet{Morris2020} computed the distribution of
the ``smoothed amplitude'',
the peak-to-trough difference of the (normalized, de-trended) light curve.
This distribution was then compared to the distribution of observed
smoothed amplitude values among stellar clusters of different ages
within an ABC algorithm, yielding approximate
posterior distributions for the hyperparameters as a function of
stellar age. \citet{Morris2020} found that the spot coverage $f_S$ as a
function of age is well modeled by a simple power law, decreasing from
${\sim}10\%$ for the youngest (${\sim}10$ Myr) stars
to less than $1\%$ for the oldest (${\sim}5$ Gyr) stars.
%
While this broadly agrees with the expectation that stellar
magnetic activity decreases over time, our work strongly suggests
that these results are predominantly prior-dominated. As we discussed
at length
in \S\ref{sec:breaking_baseline_degeneracy} (and found in our inference
runs in \S\ref{sec:calibration-inference}, as well as in the toy
model presented in Figure~\ref{fig:mean_normalization}), the fractional
spot coverage $f_S$ is not an observable in single-band photometry.
%
This is because the expression for $f_S$ (Equation~\ref{eq:fS}) depends on
two quantities: the average spot radius $r$, which we showed can be
constrained, and the total number of spots $n$, which
we showed \emph{cannot} be constrained from single-band light curves.
%
In fact, \citet{Morris2020} assumed $n=3$ for simplicity when doing
posterior inference. We therefore urge care in interpreting those results,
as it is very likely this assumption does not hold across the large
range of spectral types and stellar ages considered in that study.
Nevertheless, the power law scaling of the smoothed amplitude with
age suggests it is still a good metric for stellar ages.

Moreover, the core idea in \citet{Morris2020}
\citep[and in related studies such as][]{Jackson2013}
is very similar
to that advocated here: the use of ensemble analyses to constrain population-level
parameters when invidual datasets are not sufficiently constraining.
Our work builds on that of \citet{Morris2020} by deriving a closed form
solution to the likelihood function (as opposed to a sample-based
likelihood-free inference algorithm) and by harnessing the covariance
structure of the data when doing inference (as opposed to relying solely
on the amplitude of the data).

Another recent paper relevant to our work is that of \citet{Basri2020}, who
investigated the information content of stellar light curves, exploring what
can and cannot be learned about star spot configurations from individual
light curves. That paper strongly urges against the common practice
of interpreting
light curves with one or two dips as originating from one or two spots,
respectively, a point we strongly agree with (c.f. Figure~\ref{fig:degeneracies}).
It also reinforces our point about the additional degeneracies introduced
by the unknown baseline inherent to single-band photometry.
%
Finally, \citet{Basri2020} present a large suite of forward models of light curves of
spotted stars, which they use to discuss the (complicated) dependence of
various light curve metrics on
the physical spot parameters used to generate the data. They conclude that it
is not possible to uniquely relate these metrics to the underlying star spot
configuration. While we agree this is the case for individual stars, our work
stresses that it is possible to break many of these degeneracies with
ensemble analyses.
%
\citet{Basri2020} also conclude it is not in general possible to
uniquely disentangle differential rotation from spot evolution when their
timescales are comparable; nor is it possible to confidently measure a rotation
period when the evolution timescale is very short. However, their study
relies on the effect these processes have on simple light curve metrics,
which are almost certainly not sufficient statistics of the data. Inference
that takes into account the full covariance structure of the data, while
considering large ensembles of light curves, could in principle break
this degeneracy. While we do not explicitly model differential rotation in
this paper, it will be the subject of a future paper in this series.

\xxx{Discuss how this is somewhat similar to what \citet{Rauscher2018} did.}
\xxx{TODO...}

Nevertheless, it is still possible to uniquely and precisely infer the spot properties
under limb darkening,
but as a rule of thumb, the stronger the limb darkening, the larger the
ensemble must be.

\bibliography{bib}

\end{document}
